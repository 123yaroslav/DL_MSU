{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/123yaroslav/DL_MSU/blob/main/DL_MSU_HW07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FszmDROF12gL"
      },
      "source": [
        "## Домашнее задание №7\n",
        "### Генерация поэзии с помощью нейронных сетей: шаг 1\n",
        "##### Авторы: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), Горохов Олег\n",
        "\n",
        "Ваша основная задача: научиться генерировать стихи с помощью простой рекуррентной нейронной сети (Vanilla RNN). В качестве корпуса текстов для обучения будет выступать роман в стихах \"Евгений Онегин\" Александра Сергеевича Пушкина."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VK6EUBMt12gN"
      },
      "outputs": [],
      "source": [
        "# Не меняйте блок кода ниже! Здесь указаны все необходимые import-ы\n",
        "# __________start of block__________\n",
        "import string\n",
        "import os\n",
        "from random import sample\n",
        "\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VEC9Irlo12gO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff44685-3198-4cc7-bce7-575b3f3a42fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda device is available\n"
          ]
        }
      ],
      "source": [
        "# Не меняйте блок кода ниже!\n",
        "# __________start of block__________\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print('{} device is available'.format(device))\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPenWOy01Ooa",
        "outputId": "a92e8e33-e009-4bd4-ac12-3b1b5e1cd3f2"
      },
      "source": [
        "#### 1. Загрузка данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OhD8UJzN12gP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac0155b-3a06-419f-a8dc-4852c9168b85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-07 07:36:05--  https://raw.githubusercontent.com/MSUcourses/Data-Analysis-with-Python/main/Deep%20Learning/onegin_hw07.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 262521 (256K) [text/plain]\n",
            "Saving to: ‘./onegin.txt’\n",
            "\n",
            "./onegin.txt        100%[===================>] 256.37K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-04-07 07:36:05 (22.1 MB/s) - ‘./onegin.txt’ saved [262521/262521]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "!wget https://raw.githubusercontent.com/MSUcourses/Data-Analysis-with-Python/main/Deep%20Learning/onegin_hw07.txt -O ./onegin.txt\n",
        "\n",
        "with open('onegin.txt', 'r') as iofile:\n",
        "    text = iofile.readlines()\n",
        "\n",
        "text = \"\".join([x.replace('\\t\\t', '').lower() for x in text]) # Убираем лишние символы табуляций, приводим все буквы к нижнему регистру\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выведем несколько первых символов входного текста. Видим, что символы табуляций удалены, буквы приведены к нижнему регистру. Символы \\n мы оставляем - чтобы научить сеть генерировать символ \\n, когда нужно перейти на новую строку."
      ],
      "metadata": {
        "id": "rR6n1UmvZ8DC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text[:36]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ksrWh_hvZsY_",
        "outputId": "924039d5-6356-4ab4-b197-338edb534f5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ni\\n\\n«мой дядя самых честных правил,\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQYpmGfR_gJ8"
      },
      "source": [
        "#### 2. Построение словаря и предобработка текста\n",
        "В данном задании требуется построить языковую модель на уровне символов. Приведем весь текст к нижнему регистру и построим словарь из всех символов в доступном корпусе текстов. Также добавим технический токен `<sos>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FyYTS2kC12gQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc0dcddd-2eea-47d6-b38d-ede49f3bcb43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seems fine!\n"
          ]
        }
      ],
      "source": [
        "# Не меняйте блок кода ниже!\n",
        "# __________start of block__________\n",
        "tokens = sorted(set(text.lower())) + ['<sos>'] # Строим множество всех токенов-символов и добавляем к нему служебный токен <sos>\n",
        "num_tokens = len(tokens)\n",
        "\n",
        "assert num_tokens == 84, \"Check the tokenization process\"\n",
        "\n",
        "token_to_idx = {x: idx for idx, x in enumerate(tokens)} # Строим словарь с ключами-токенами и значениями-индексами в списке токенов\n",
        "idx_to_token = {idx: x for idx, x in enumerate(tokens)} # Строим обратный словарь (чтобы по индексу можно было получить токен)\n",
        "\n",
        "assert len(tokens) == len(token_to_idx), \"Mapping should be unique\"\n",
        "\n",
        "print(\"Seems fine!\")\n",
        "\n",
        "\n",
        "text_encoded = [token_to_idx[x] for x in text]\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITona8rr12gQ"
      },
      "source": [
        "__Ваша задача__: обучить классическую рекуррентную нейронную сеть (Vanilla RNN) предсказывать следующий символ на полученном корпусе текстов и сгенерировать последовательность длины 100 для фиксированной начальной фразы.\n",
        "\n",
        "Вы можете воспользоваться кодом с занятия №6 или же обратиться к следующим ссылкам:\n",
        "* Замечательная статья за авторством Andrej Karpathy об использовании RNN: [link](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "* Пример char-rnn от Andrej Karpathy: [github repo](https://github.com/karpathy/char-rnn)\n",
        "* Замечательный пример генерации поэзии Шекспира: [github repo](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb)\n",
        "\n",
        "Данное задание является достаточно творческим. Не страшно, если поначалу оно вызывает затруднения. Последняя ссылка в списке выше может быть особенно полезна в данном случае.\n",
        "\n",
        "Далее для вашего удобства реализована функция, которая генерирует случайный батч размера `batch_size` из строк длиной `seq_length`. Вы можете использовать его при обучении модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B1R2KoN212gQ"
      },
      "outputs": [],
      "source": [
        "# Не меняйте код ниже\n",
        "# __________start of block__________\n",
        "batch_size = 256 # Размер батча. Батч - это набор последовательностей символов.\n",
        "seq_length = 100 # Максимальная длина одной последовательности символов в батче\n",
        "start_column = np.zeros((batch_size, 1), dtype=int) + token_to_idx['<sos>'] # Добавляем в начало каждой строки технический символ - для определения начального состояния сети\n",
        "\n",
        "def generate_chunk():\n",
        "    global text_encoded, start_column, batch_size, seq_length\n",
        "\n",
        "    start_index = np.random.randint(0, len(text_encoded) - batch_size*seq_length - 1) # Случайным образом выбираем индекс начального символа в батче\n",
        "    # Строим непрерывный батч.\n",
        "    # Для этого выбираем в исходном тексте подпоследовательность, которая начинается с индекса start_index и имеет размер batch_size*seq_length.\n",
        "    # Затем мы делим эту подпоследовательность на batch_size последовательностей размера seq_length. Это и будет батч, матрица размера batch_size*seq_length.\n",
        "    # В каждой строке матрицы будут указаны индексы\n",
        "    data = np.array(text_encoded[start_index:start_index + batch_size*seq_length]).reshape((batch_size, -1))\n",
        "    yield np.hstack((start_column, data))\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHohQt0V12gR"
      },
      "source": [
        "Пример батча:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jyuWYgeZ12gR",
        "outputId": "e69180d3-b888-4c00-9cb7-9f3e4f71ce57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[83,  1, 46, ...,  0, 52, 49],\n",
              "       [83, 50, 62, ..., 51, 45, 56],\n",
              "       [83, 64, 54, ..., 53,  1, 55],\n",
              "       ...,\n",
              "       [83, 58, 53, ..., 59, 57,  1],\n",
              "       [83, 47, 62, ..., 50, 63, 73],\n",
              "       [83,  1, 59, ...,  0, 47, 62]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "next(generate_chunk())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ5vfEKM12gS"
      },
      "source": [
        "Далее вам предстоит написать код для обучения модели и генерации текста."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7XBmwWxp12gS"
      },
      "outputs": [],
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, num_classes, embedding_dim=128, hidden_size=256, n_layers=1):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(num_classes, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_size, n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        out, hidden = self.rnn(embedded, hidden)\n",
        "\n",
        "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
        "\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device)\n",
        "        return hidden\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOq8SszD12gS"
      },
      "source": [
        "В качестве иллюстрации ниже доступен график значений функции потерь, построенный в ходе обучения авторской сети (сам код для ее обучения вам и предстоит написать)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = CharRNN(num_tokens, embedding_dim=128, hidden_size=256, n_layers=2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "n_epochs = 10000\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    for batch in generate_chunk():\n",
        "        inputs, targets = batch[:, :-1], batch[:, 1:]\n",
        "        inputs, targets = torch.LongTensor(inputs).to(device), torch.LongTensor(targets).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output, hidden = model(inputs, hidden.detach())\n",
        "\n",
        "        loss = criterion(output, targets.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
        "\n"
      ],
      "metadata": {
        "id": "0PMvQ29lfiE1",
        "outputId": "92752d26-f0a0-4e94-c35b-d89586a838f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 4.442775726318359\n",
            "Epoch: 10, Loss: 2.738335609436035\n",
            "Epoch: 20, Loss: 2.4892454147338867\n",
            "Epoch: 30, Loss: 2.3867037296295166\n",
            "Epoch: 40, Loss: 2.3131661415100098\n",
            "Epoch: 50, Loss: 2.2136638164520264\n",
            "Epoch: 60, Loss: 2.183074951171875\n",
            "Epoch: 70, Loss: 2.0953595638275146\n",
            "Epoch: 80, Loss: 2.014157772064209\n",
            "Epoch: 90, Loss: 2.0238277912139893\n",
            "Epoch: 100, Loss: 1.979306936264038\n",
            "Epoch: 110, Loss: 1.902870774269104\n",
            "Epoch: 120, Loss: 1.9894479513168335\n",
            "Epoch: 130, Loss: 1.9420669078826904\n",
            "Epoch: 140, Loss: 1.7641280889511108\n",
            "Epoch: 150, Loss: 1.7651162147521973\n",
            "Epoch: 160, Loss: 1.746534824371338\n",
            "Epoch: 170, Loss: 1.6218143701553345\n",
            "Epoch: 180, Loss: 1.63326096534729\n",
            "Epoch: 190, Loss: 1.646106243133545\n",
            "Epoch: 200, Loss: 1.5882060527801514\n",
            "Epoch: 210, Loss: 1.5015379190444946\n",
            "Epoch: 220, Loss: 1.6467311382293701\n",
            "Epoch: 230, Loss: 1.4689384698867798\n",
            "Epoch: 240, Loss: 1.5127537250518799\n",
            "Epoch: 250, Loss: 1.4545437097549438\n",
            "Epoch: 260, Loss: 1.3896716833114624\n",
            "Epoch: 270, Loss: 1.4939579963684082\n",
            "Epoch: 280, Loss: 1.4688050746917725\n",
            "Epoch: 290, Loss: 1.4062738418579102\n",
            "Epoch: 300, Loss: 1.2547820806503296\n",
            "Epoch: 310, Loss: 1.1895911693572998\n",
            "Epoch: 320, Loss: 1.2314753532409668\n",
            "Epoch: 330, Loss: 1.274325966835022\n",
            "Epoch: 340, Loss: 1.1631027460098267\n",
            "Epoch: 350, Loss: 1.2857344150543213\n",
            "Epoch: 360, Loss: 1.2281863689422607\n",
            "Epoch: 370, Loss: 1.5800284147262573\n",
            "Epoch: 380, Loss: 1.2142082452774048\n",
            "Epoch: 390, Loss: 1.3063921928405762\n",
            "Epoch: 400, Loss: 1.0966103076934814\n",
            "Epoch: 410, Loss: 1.1218374967575073\n",
            "Epoch: 420, Loss: 1.0031845569610596\n",
            "Epoch: 430, Loss: 1.0145025253295898\n",
            "Epoch: 440, Loss: 1.5488473176956177\n",
            "Epoch: 450, Loss: 1.0754848718643188\n",
            "Epoch: 460, Loss: 0.9443795084953308\n",
            "Epoch: 470, Loss: 0.8146806955337524\n",
            "Epoch: 480, Loss: 0.9785829782485962\n",
            "Epoch: 490, Loss: 0.7475043535232544\n",
            "Epoch: 500, Loss: 1.172015905380249\n",
            "Epoch: 510, Loss: 1.0427948236465454\n",
            "Epoch: 520, Loss: 0.8672370910644531\n",
            "Epoch: 530, Loss: 1.136610746383667\n",
            "Epoch: 540, Loss: 0.8595505356788635\n",
            "Epoch: 550, Loss: 0.8289536833763123\n",
            "Epoch: 560, Loss: 0.7885025143623352\n",
            "Epoch: 570, Loss: 0.7226908802986145\n",
            "Epoch: 580, Loss: 0.6860547661781311\n",
            "Epoch: 590, Loss: 0.7122053503990173\n",
            "Epoch: 600, Loss: 0.750571072101593\n",
            "Epoch: 610, Loss: 0.8506019711494446\n",
            "Epoch: 620, Loss: 0.7485119700431824\n",
            "Epoch: 630, Loss: 0.8483502268791199\n",
            "Epoch: 640, Loss: 1.1705389022827148\n",
            "Epoch: 650, Loss: 0.8489035964012146\n",
            "Epoch: 660, Loss: 0.8128926157951355\n",
            "Epoch: 670, Loss: 0.6882219910621643\n",
            "Epoch: 680, Loss: 0.6559801697731018\n",
            "Epoch: 690, Loss: 0.8107351660728455\n",
            "Epoch: 700, Loss: 0.7117050290107727\n",
            "Epoch: 710, Loss: 0.6732017993927002\n",
            "Epoch: 720, Loss: 0.7205039262771606\n",
            "Epoch: 730, Loss: 0.588333785533905\n",
            "Epoch: 740, Loss: 0.7524377703666687\n",
            "Epoch: 750, Loss: 0.6856656074523926\n",
            "Epoch: 760, Loss: 0.6872531771659851\n",
            "Epoch: 770, Loss: 0.5422871112823486\n",
            "Epoch: 780, Loss: 1.406585693359375\n",
            "Epoch: 790, Loss: 0.7018460631370544\n",
            "Epoch: 800, Loss: 0.7852888703346252\n",
            "Epoch: 810, Loss: 1.1497305631637573\n",
            "Epoch: 820, Loss: 0.6816256642341614\n",
            "Epoch: 830, Loss: 0.6803346276283264\n",
            "Epoch: 840, Loss: 0.7760540246963501\n",
            "Epoch: 850, Loss: 0.673908531665802\n",
            "Epoch: 860, Loss: 1.0849637985229492\n",
            "Epoch: 870, Loss: 0.5745596289634705\n",
            "Epoch: 880, Loss: 0.6724474430084229\n",
            "Epoch: 890, Loss: 0.5439342856407166\n",
            "Epoch: 900, Loss: 0.609057605266571\n",
            "Epoch: 910, Loss: 0.6652491092681885\n",
            "Epoch: 920, Loss: 0.5794447064399719\n",
            "Epoch: 930, Loss: 0.6494821310043335\n",
            "Epoch: 940, Loss: 0.5445829033851624\n",
            "Epoch: 950, Loss: 0.6463565826416016\n",
            "Epoch: 960, Loss: 0.55806565284729\n",
            "Epoch: 970, Loss: 0.5434964895248413\n",
            "Epoch: 980, Loss: 0.6108620166778564\n",
            "Epoch: 990, Loss: 0.5260005593299866\n",
            "Epoch: 1000, Loss: 0.5245566368103027\n",
            "Epoch: 1010, Loss: 0.9320701360702515\n",
            "Epoch: 1020, Loss: 0.544498085975647\n",
            "Epoch: 1030, Loss: 0.5324358344078064\n",
            "Epoch: 1040, Loss: 0.663017749786377\n",
            "Epoch: 1050, Loss: 1.2219525575637817\n",
            "Epoch: 1060, Loss: 1.128664255142212\n",
            "Epoch: 1070, Loss: 0.5211513638496399\n",
            "Epoch: 1080, Loss: 0.7681114077568054\n",
            "Epoch: 1090, Loss: 0.6809179186820984\n",
            "Epoch: 1100, Loss: 0.5755022168159485\n",
            "Epoch: 1110, Loss: 0.5828199982643127\n",
            "Epoch: 1120, Loss: 0.8893390893936157\n",
            "Epoch: 1130, Loss: 0.8989036679267883\n",
            "Epoch: 1140, Loss: 0.5567358136177063\n",
            "Epoch: 1150, Loss: 0.5068603754043579\n",
            "Epoch: 1160, Loss: 0.5987539291381836\n",
            "Epoch: 1170, Loss: 0.47386059165000916\n",
            "Epoch: 1180, Loss: 0.5771380662918091\n",
            "Epoch: 1190, Loss: 0.6898722648620605\n",
            "Epoch: 1200, Loss: 0.5055844187736511\n",
            "Epoch: 1210, Loss: 0.8460085391998291\n",
            "Epoch: 1220, Loss: 0.5257901549339294\n",
            "Epoch: 1230, Loss: 0.5148258805274963\n",
            "Epoch: 1240, Loss: 0.601101279258728\n",
            "Epoch: 1250, Loss: 0.47981151938438416\n",
            "Epoch: 1260, Loss: 0.6001833081245422\n",
            "Epoch: 1270, Loss: 0.5877903699874878\n",
            "Epoch: 1280, Loss: 0.5642191171646118\n",
            "Epoch: 1290, Loss: 0.5718318819999695\n",
            "Epoch: 1300, Loss: 0.680176317691803\n",
            "Epoch: 1310, Loss: 0.5515886545181274\n",
            "Epoch: 1320, Loss: 0.8615335822105408\n",
            "Epoch: 1330, Loss: 1.213147759437561\n",
            "Epoch: 1340, Loss: 0.576123833656311\n",
            "Epoch: 1350, Loss: 0.47541362047195435\n",
            "Epoch: 1360, Loss: 0.5671354532241821\n",
            "Epoch: 1370, Loss: 0.7209346294403076\n",
            "Epoch: 1380, Loss: 1.4966528415679932\n",
            "Epoch: 1390, Loss: 0.7681921124458313\n",
            "Epoch: 1400, Loss: 0.5678861737251282\n",
            "Epoch: 1410, Loss: 0.5714068412780762\n",
            "Epoch: 1420, Loss: 0.5235071778297424\n",
            "Epoch: 1430, Loss: 0.5598373413085938\n",
            "Epoch: 1440, Loss: 0.7599717974662781\n",
            "Epoch: 1450, Loss: 0.7594065070152283\n",
            "Epoch: 1460, Loss: 0.5235059261322021\n",
            "Epoch: 1470, Loss: 0.6795662045478821\n",
            "Epoch: 1480, Loss: 0.7125317454338074\n",
            "Epoch: 1490, Loss: 0.542127251625061\n",
            "Epoch: 1500, Loss: 0.5105466246604919\n",
            "Epoch: 1510, Loss: 0.4727323651313782\n",
            "Epoch: 1520, Loss: 0.5572423934936523\n",
            "Epoch: 1530, Loss: 0.9140313863754272\n",
            "Epoch: 1540, Loss: 0.5717620253562927\n",
            "Epoch: 1550, Loss: 0.5290731191635132\n",
            "Epoch: 1560, Loss: 0.4844353497028351\n",
            "Epoch: 1570, Loss: 0.4944102168083191\n",
            "Epoch: 1580, Loss: 0.8363766670227051\n",
            "Epoch: 1590, Loss: 1.1916823387145996\n",
            "Epoch: 1600, Loss: 0.5200440883636475\n",
            "Epoch: 1610, Loss: 0.5136949419975281\n",
            "Epoch: 1620, Loss: 0.5125722289085388\n",
            "Epoch: 1630, Loss: 0.5724121332168579\n",
            "Epoch: 1640, Loss: 0.881162703037262\n",
            "Epoch: 1650, Loss: 0.5589914917945862\n",
            "Epoch: 1660, Loss: 0.8493788838386536\n",
            "Epoch: 1670, Loss: 0.7282390594482422\n",
            "Epoch: 1680, Loss: 1.0688345432281494\n",
            "Epoch: 1690, Loss: 0.5624527931213379\n",
            "Epoch: 1700, Loss: 0.5384927988052368\n",
            "Epoch: 1710, Loss: 0.5491291284561157\n",
            "Epoch: 1720, Loss: 0.5331652164459229\n",
            "Epoch: 1730, Loss: 1.408819556236267\n",
            "Epoch: 1740, Loss: 0.6635668873786926\n",
            "Epoch: 1750, Loss: 0.5032345056533813\n",
            "Epoch: 1760, Loss: 0.5771063566207886\n",
            "Epoch: 1770, Loss: 1.0004489421844482\n",
            "Epoch: 1780, Loss: 0.5100885033607483\n",
            "Epoch: 1790, Loss: 0.5497053265571594\n",
            "Epoch: 1800, Loss: 0.530041515827179\n",
            "Epoch: 1810, Loss: 0.6342606544494629\n",
            "Epoch: 1820, Loss: 0.5905381441116333\n",
            "Epoch: 1830, Loss: 0.6012659668922424\n",
            "Epoch: 1840, Loss: 0.6817520260810852\n",
            "Epoch: 1850, Loss: 0.47455647587776184\n",
            "Epoch: 1860, Loss: 0.5250087976455688\n",
            "Epoch: 1870, Loss: 0.4758699834346771\n",
            "Epoch: 1880, Loss: 0.8507642149925232\n",
            "Epoch: 1890, Loss: 0.4599120616912842\n",
            "Epoch: 1900, Loss: 0.4620477557182312\n",
            "Epoch: 1910, Loss: 0.481901079416275\n",
            "Epoch: 1920, Loss: 0.5149431228637695\n",
            "Epoch: 1930, Loss: 0.48179078102111816\n",
            "Epoch: 1940, Loss: 0.5356965065002441\n",
            "Epoch: 1950, Loss: 0.44862592220306396\n",
            "Epoch: 1960, Loss: 0.4975903332233429\n",
            "Epoch: 1970, Loss: 0.5614394545555115\n",
            "Epoch: 1980, Loss: 0.6662423014640808\n",
            "Epoch: 1990, Loss: 1.0618069171905518\n",
            "Epoch: 2000, Loss: 0.5160150527954102\n",
            "Epoch: 2010, Loss: 0.5698954463005066\n",
            "Epoch: 2020, Loss: 0.5045744180679321\n",
            "Epoch: 2030, Loss: 0.6481879353523254\n",
            "Epoch: 2040, Loss: 0.5494608283042908\n",
            "Epoch: 2050, Loss: 0.46852099895477295\n",
            "Epoch: 2060, Loss: 0.45312735438346863\n",
            "Epoch: 2070, Loss: 0.5029318332672119\n",
            "Epoch: 2080, Loss: 0.46945858001708984\n",
            "Epoch: 2090, Loss: 0.5217511057853699\n",
            "Epoch: 2100, Loss: 0.4878195524215698\n",
            "Epoch: 2110, Loss: 0.49064522981643677\n",
            "Epoch: 2120, Loss: 0.4649433493614197\n",
            "Epoch: 2130, Loss: 0.48094460368156433\n",
            "Epoch: 2140, Loss: 0.5318458676338196\n",
            "Epoch: 2150, Loss: 0.482122540473938\n",
            "Epoch: 2160, Loss: 0.5087252259254456\n",
            "Epoch: 2170, Loss: 0.6812844276428223\n",
            "Epoch: 2180, Loss: 0.5560622811317444\n",
            "Epoch: 2190, Loss: 0.45044827461242676\n",
            "Epoch: 2200, Loss: 0.46369102597236633\n",
            "Epoch: 2210, Loss: 0.52339768409729\n",
            "Epoch: 2220, Loss: 0.5089470148086548\n",
            "Epoch: 2230, Loss: 0.3791649341583252\n",
            "Epoch: 2240, Loss: 0.4826209247112274\n",
            "Epoch: 2250, Loss: 0.4722956717014313\n",
            "Epoch: 2260, Loss: 1.1773391962051392\n",
            "Epoch: 2270, Loss: 0.5628820657730103\n",
            "Epoch: 2280, Loss: 0.4580574333667755\n",
            "Epoch: 2290, Loss: 0.4473980665206909\n",
            "Epoch: 2300, Loss: 0.4844079613685608\n",
            "Epoch: 2310, Loss: 0.4990009367465973\n",
            "Epoch: 2320, Loss: 0.4619952440261841\n",
            "Epoch: 2330, Loss: 0.4325698912143707\n",
            "Epoch: 2340, Loss: 0.41088366508483887\n",
            "Epoch: 2350, Loss: 0.42635560035705566\n",
            "Epoch: 2360, Loss: 0.4818143844604492\n",
            "Epoch: 2370, Loss: 0.5126131176948547\n",
            "Epoch: 2380, Loss: 1.1288567781448364\n",
            "Epoch: 2390, Loss: 0.49158793687820435\n",
            "Epoch: 2400, Loss: 0.7555719017982483\n",
            "Epoch: 2410, Loss: 0.4535675346851349\n",
            "Epoch: 2420, Loss: 0.6231290102005005\n",
            "Epoch: 2430, Loss: 0.5600733160972595\n",
            "Epoch: 2440, Loss: 0.7808045148849487\n",
            "Epoch: 2450, Loss: 0.7387866973876953\n",
            "Epoch: 2460, Loss: 0.4526844918727875\n",
            "Epoch: 2470, Loss: 0.4680352807044983\n",
            "Epoch: 2480, Loss: 0.5293669700622559\n",
            "Epoch: 2490, Loss: 0.4681088924407959\n",
            "Epoch: 2500, Loss: 0.45738428831100464\n",
            "Epoch: 2510, Loss: 0.4492086172103882\n",
            "Epoch: 2520, Loss: 0.4110376834869385\n",
            "Epoch: 2530, Loss: 0.47327426075935364\n",
            "Epoch: 2540, Loss: 0.4490354061126709\n",
            "Epoch: 2550, Loss: 0.4661550521850586\n",
            "Epoch: 2560, Loss: 0.48486602306365967\n",
            "Epoch: 2570, Loss: 0.5514827966690063\n",
            "Epoch: 2580, Loss: 0.760520875453949\n",
            "Epoch: 2590, Loss: 0.4799731373786926\n",
            "Epoch: 2600, Loss: 0.492946982383728\n",
            "Epoch: 2610, Loss: 0.3978962004184723\n",
            "Epoch: 2620, Loss: 0.42690509557724\n",
            "Epoch: 2630, Loss: 0.49023112654685974\n",
            "Epoch: 2640, Loss: 0.5361618399620056\n",
            "Epoch: 2650, Loss: 0.4791547358036041\n",
            "Epoch: 2660, Loss: 0.4839082956314087\n",
            "Epoch: 2670, Loss: 0.4891546666622162\n",
            "Epoch: 2680, Loss: 0.54210364818573\n",
            "Epoch: 2690, Loss: 0.42340990900993347\n",
            "Epoch: 2700, Loss: 0.5055630803108215\n",
            "Epoch: 2710, Loss: 0.4239535629749298\n",
            "Epoch: 2720, Loss: 1.0240379571914673\n",
            "Epoch: 2730, Loss: 0.4919126033782959\n",
            "Epoch: 2740, Loss: 0.7850616574287415\n",
            "Epoch: 2750, Loss: 0.5952662825584412\n",
            "Epoch: 2760, Loss: 0.45206981897354126\n",
            "Epoch: 2770, Loss: 0.4450879693031311\n",
            "Epoch: 2780, Loss: 1.0007320642471313\n",
            "Epoch: 2790, Loss: 0.4575396776199341\n",
            "Epoch: 2800, Loss: 0.4583357870578766\n",
            "Epoch: 2810, Loss: 0.44634732604026794\n",
            "Epoch: 2820, Loss: 0.43878546357154846\n",
            "Epoch: 2830, Loss: 0.43937361240386963\n",
            "Epoch: 2840, Loss: 1.0076591968536377\n",
            "Epoch: 2850, Loss: 0.4260019063949585\n",
            "Epoch: 2860, Loss: 0.463628888130188\n",
            "Epoch: 2870, Loss: 0.46543893218040466\n",
            "Epoch: 2880, Loss: 0.4886173903942108\n",
            "Epoch: 2890, Loss: 0.46267709136009216\n",
            "Epoch: 2900, Loss: 0.44701695442199707\n",
            "Epoch: 2910, Loss: 0.520189642906189\n",
            "Epoch: 2920, Loss: 0.42184218764305115\n",
            "Epoch: 2930, Loss: 0.4574669599533081\n",
            "Epoch: 2940, Loss: 0.42822179198265076\n",
            "Epoch: 2950, Loss: 0.39851707220077515\n",
            "Epoch: 2960, Loss: 0.37489739060401917\n",
            "Epoch: 2970, Loss: 0.4052087664604187\n",
            "Epoch: 2980, Loss: 0.7445725202560425\n",
            "Epoch: 2990, Loss: 0.4596899747848511\n",
            "Epoch: 3000, Loss: 0.4491198658943176\n",
            "Epoch: 3010, Loss: 0.5613851547241211\n",
            "Epoch: 3020, Loss: 0.44377976655960083\n",
            "Epoch: 3030, Loss: 0.4759690761566162\n",
            "Epoch: 3040, Loss: 0.4712158143520355\n",
            "Epoch: 3050, Loss: 0.4145312011241913\n",
            "Epoch: 3060, Loss: 0.48740673065185547\n",
            "Epoch: 3070, Loss: 0.43580707907676697\n",
            "Epoch: 3080, Loss: 0.7113980650901794\n",
            "Epoch: 3090, Loss: 0.5332194566726685\n",
            "Epoch: 3100, Loss: 0.4625810384750366\n",
            "Epoch: 3110, Loss: 0.43320804834365845\n",
            "Epoch: 3120, Loss: 0.6045250296592712\n",
            "Epoch: 3130, Loss: 0.9301259517669678\n",
            "Epoch: 3140, Loss: 0.4964290261268616\n",
            "Epoch: 3150, Loss: 0.430780827999115\n",
            "Epoch: 3160, Loss: 0.5018054842948914\n",
            "Epoch: 3170, Loss: 0.5069583058357239\n",
            "Epoch: 3180, Loss: 0.583613395690918\n",
            "Epoch: 3190, Loss: 0.5790902376174927\n",
            "Epoch: 3200, Loss: 0.49965953826904297\n",
            "Epoch: 3210, Loss: 0.46856608986854553\n",
            "Epoch: 3220, Loss: 0.8249468207359314\n",
            "Epoch: 3230, Loss: 0.45700523257255554\n",
            "Epoch: 3240, Loss: 0.8565657138824463\n",
            "Epoch: 3250, Loss: 1.0360127687454224\n",
            "Epoch: 3260, Loss: 0.47204822301864624\n",
            "Epoch: 3270, Loss: 0.4916834235191345\n",
            "Epoch: 3280, Loss: 0.9236112833023071\n",
            "Epoch: 3290, Loss: 0.46130266785621643\n",
            "Epoch: 3300, Loss: 0.44060492515563965\n",
            "Epoch: 3310, Loss: 0.616641104221344\n",
            "Epoch: 3320, Loss: 0.4677608609199524\n",
            "Epoch: 3330, Loss: 0.4180130362510681\n",
            "Epoch: 3340, Loss: 0.4769773483276367\n",
            "Epoch: 3350, Loss: 0.43855586647987366\n",
            "Epoch: 3360, Loss: 0.42987099289894104\n",
            "Epoch: 3370, Loss: 0.44097575545310974\n",
            "Epoch: 3380, Loss: 0.46849745512008667\n",
            "Epoch: 3390, Loss: 0.4116365909576416\n",
            "Epoch: 3400, Loss: 0.9854210615158081\n",
            "Epoch: 3410, Loss: 0.4641837775707245\n",
            "Epoch: 3420, Loss: 0.6043652892112732\n",
            "Epoch: 3430, Loss: 0.48722025752067566\n",
            "Epoch: 3440, Loss: 0.41894903779029846\n",
            "Epoch: 3450, Loss: 0.4588755667209625\n",
            "Epoch: 3460, Loss: 0.38564926385879517\n",
            "Epoch: 3470, Loss: 0.5008862018585205\n",
            "Epoch: 3480, Loss: 0.77601158618927\n",
            "Epoch: 3490, Loss: 0.4524417519569397\n",
            "Epoch: 3500, Loss: 0.44891157746315\n",
            "Epoch: 3510, Loss: 0.4582461416721344\n",
            "Epoch: 3520, Loss: 0.5211708545684814\n",
            "Epoch: 3530, Loss: 0.440784752368927\n",
            "Epoch: 3540, Loss: 0.6114834547042847\n",
            "Epoch: 3550, Loss: 0.4942341148853302\n",
            "Epoch: 3560, Loss: 0.4763960540294647\n",
            "Epoch: 3570, Loss: 0.5008432865142822\n",
            "Epoch: 3580, Loss: 0.5050256848335266\n",
            "Epoch: 3590, Loss: 0.6486353874206543\n",
            "Epoch: 3600, Loss: 1.0193846225738525\n",
            "Epoch: 3610, Loss: 0.45658496022224426\n",
            "Epoch: 3620, Loss: 0.5280815958976746\n",
            "Epoch: 3630, Loss: 0.46251314878463745\n",
            "Epoch: 3640, Loss: 0.4191089868545532\n",
            "Epoch: 3650, Loss: 0.39615383744239807\n",
            "Epoch: 3660, Loss: 0.440164715051651\n",
            "Epoch: 3670, Loss: 0.4557816684246063\n",
            "Epoch: 3680, Loss: 0.45907893776893616\n",
            "Epoch: 3690, Loss: 0.4735003709793091\n",
            "Epoch: 3700, Loss: 0.43915989995002747\n",
            "Epoch: 3710, Loss: 0.3954838216304779\n",
            "Epoch: 3720, Loss: 0.7464659810066223\n",
            "Epoch: 3730, Loss: 0.45592108368873596\n",
            "Epoch: 3740, Loss: 0.4075612723827362\n",
            "Epoch: 3750, Loss: 0.47105705738067627\n",
            "Epoch: 3760, Loss: 0.42847877740859985\n",
            "Epoch: 3770, Loss: 0.4610455334186554\n",
            "Epoch: 3780, Loss: 0.4835042655467987\n",
            "Epoch: 3790, Loss: 0.4791363775730133\n",
            "Epoch: 3800, Loss: 0.42492496967315674\n",
            "Epoch: 3810, Loss: 0.5108192563056946\n",
            "Epoch: 3820, Loss: 0.5875425934791565\n",
            "Epoch: 3830, Loss: 0.46888720989227295\n",
            "Epoch: 3840, Loss: 0.4272163510322571\n",
            "Epoch: 3850, Loss: 0.4457418918609619\n",
            "Epoch: 3860, Loss: 0.4469121992588043\n",
            "Epoch: 3870, Loss: 0.4486500918865204\n",
            "Epoch: 3880, Loss: 0.4713139235973358\n",
            "Epoch: 3890, Loss: 0.5676737427711487\n",
            "Epoch: 3900, Loss: 0.4744810163974762\n",
            "Epoch: 3910, Loss: 0.5370128154754639\n",
            "Epoch: 3920, Loss: 0.5329245924949646\n",
            "Epoch: 3930, Loss: 0.48186373710632324\n",
            "Epoch: 3940, Loss: 0.4353276491165161\n",
            "Epoch: 3950, Loss: 0.39611902832984924\n",
            "Epoch: 3960, Loss: 0.4036425054073334\n",
            "Epoch: 3970, Loss: 0.4222775399684906\n",
            "Epoch: 3980, Loss: 0.42929428815841675\n",
            "Epoch: 3990, Loss: 0.5496034622192383\n",
            "Epoch: 4000, Loss: 0.4798290729522705\n",
            "Epoch: 4010, Loss: 0.5677970051765442\n",
            "Epoch: 4020, Loss: 0.938301146030426\n",
            "Epoch: 4030, Loss: 0.48001888394355774\n",
            "Epoch: 4040, Loss: 0.4267222285270691\n",
            "Epoch: 4050, Loss: 0.43037155270576477\n",
            "Epoch: 4060, Loss: 0.4867285192012787\n",
            "Epoch: 4070, Loss: 0.40238064527511597\n",
            "Epoch: 4080, Loss: 0.7261813282966614\n",
            "Epoch: 4090, Loss: 1.0611562728881836\n",
            "Epoch: 4100, Loss: 0.6531720757484436\n",
            "Epoch: 4110, Loss: 0.47324657440185547\n",
            "Epoch: 4120, Loss: 0.40034037828445435\n",
            "Epoch: 4130, Loss: 0.4127240478992462\n",
            "Epoch: 4140, Loss: 0.4608534872531891\n",
            "Epoch: 4150, Loss: 0.4189421534538269\n",
            "Epoch: 4160, Loss: 0.49367499351501465\n",
            "Epoch: 4170, Loss: 0.41567206382751465\n",
            "Epoch: 4180, Loss: 0.5587628483772278\n",
            "Epoch: 4190, Loss: 1.167803168296814\n",
            "Epoch: 4200, Loss: 1.0810874700546265\n",
            "Epoch: 4210, Loss: 0.4068566858768463\n",
            "Epoch: 4220, Loss: 0.5443007349967957\n",
            "Epoch: 4230, Loss: 0.5787757635116577\n",
            "Epoch: 4240, Loss: 0.7984111309051514\n",
            "Epoch: 4250, Loss: 0.5412369966506958\n",
            "Epoch: 4260, Loss: 0.5257284045219421\n",
            "Epoch: 4270, Loss: 0.47219642996788025\n",
            "Epoch: 4280, Loss: 0.47333359718322754\n",
            "Epoch: 4290, Loss: 0.46891623735427856\n",
            "Epoch: 4300, Loss: 0.44639167189598083\n",
            "Epoch: 4310, Loss: 0.592608630657196\n",
            "Epoch: 4320, Loss: 0.5690269470214844\n",
            "Epoch: 4330, Loss: 0.7373490333557129\n",
            "Epoch: 4340, Loss: 0.45535382628440857\n",
            "Epoch: 4350, Loss: 0.42535606026649475\n",
            "Epoch: 4360, Loss: 0.47064208984375\n",
            "Epoch: 4370, Loss: 0.3916243314743042\n",
            "Epoch: 4380, Loss: 0.5056144595146179\n",
            "Epoch: 4390, Loss: 0.40449059009552\n",
            "Epoch: 4400, Loss: 0.6297600865364075\n",
            "Epoch: 4410, Loss: 0.4983415901660919\n",
            "Epoch: 4420, Loss: 0.4440833628177643\n",
            "Epoch: 4430, Loss: 0.39893245697021484\n",
            "Epoch: 4440, Loss: 0.4559222161769867\n",
            "Epoch: 4450, Loss: 0.9723297357559204\n",
            "Epoch: 4460, Loss: 0.4732731282711029\n",
            "Epoch: 4470, Loss: 0.4920865297317505\n",
            "Epoch: 4480, Loss: 0.42438971996307373\n",
            "Epoch: 4490, Loss: 0.44037148356437683\n",
            "Epoch: 4500, Loss: 0.4376351833343506\n",
            "Epoch: 4510, Loss: 0.4331821799278259\n",
            "Epoch: 4520, Loss: 0.4131951928138733\n",
            "Epoch: 4530, Loss: 0.4342964291572571\n",
            "Epoch: 4540, Loss: 0.3835901916027069\n",
            "Epoch: 4550, Loss: 0.41463905572891235\n",
            "Epoch: 4560, Loss: 1.12027907371521\n",
            "Epoch: 4570, Loss: 0.7332001328468323\n",
            "Epoch: 4580, Loss: 0.48917147517204285\n",
            "Epoch: 4590, Loss: 0.46351945400238037\n",
            "Epoch: 4600, Loss: 0.9877291321754456\n",
            "Epoch: 4610, Loss: 0.45874249935150146\n",
            "Epoch: 4620, Loss: 0.5597541332244873\n",
            "Epoch: 4630, Loss: 0.5910037159919739\n",
            "Epoch: 4640, Loss: 0.42198872566223145\n",
            "Epoch: 4650, Loss: 0.5445006489753723\n",
            "Epoch: 4660, Loss: 0.437987357378006\n",
            "Epoch: 4670, Loss: 0.4172908067703247\n",
            "Epoch: 4680, Loss: 0.4272083342075348\n",
            "Epoch: 4690, Loss: 0.4237382411956787\n",
            "Epoch: 4700, Loss: 0.8182725310325623\n",
            "Epoch: 4710, Loss: 0.4488760232925415\n",
            "Epoch: 4720, Loss: 0.5730105042457581\n",
            "Epoch: 4730, Loss: 0.450142502784729\n",
            "Epoch: 4740, Loss: 0.4712505638599396\n",
            "Epoch: 4750, Loss: 0.9057489037513733\n",
            "Epoch: 4760, Loss: 0.4940072000026703\n",
            "Epoch: 4770, Loss: 0.504810631275177\n",
            "Epoch: 4780, Loss: 0.7195883989334106\n",
            "Epoch: 4790, Loss: 0.424215167760849\n",
            "Epoch: 4800, Loss: 0.429500550031662\n",
            "Epoch: 4810, Loss: 0.4910321831703186\n",
            "Epoch: 4820, Loss: 0.4237985908985138\n",
            "Epoch: 4830, Loss: 0.46490153670310974\n",
            "Epoch: 4840, Loss: 0.5048946142196655\n",
            "Epoch: 4850, Loss: 0.5002098679542542\n",
            "Epoch: 4860, Loss: 0.4525831937789917\n",
            "Epoch: 4870, Loss: 0.4319548010826111\n",
            "Epoch: 4880, Loss: 0.4111047685146332\n",
            "Epoch: 4890, Loss: 0.48552218079566956\n",
            "Epoch: 4900, Loss: 0.4264819324016571\n",
            "Epoch: 4910, Loss: 0.8132436275482178\n",
            "Epoch: 4920, Loss: 0.45810654759407043\n",
            "Epoch: 4930, Loss: 1.1950799226760864\n",
            "Epoch: 4940, Loss: 0.6478467583656311\n",
            "Epoch: 4950, Loss: 0.4774872958660126\n",
            "Epoch: 4960, Loss: 0.4691663384437561\n",
            "Epoch: 4970, Loss: 0.48775777220726013\n",
            "Epoch: 4980, Loss: 0.4217389225959778\n",
            "Epoch: 4990, Loss: 0.4173470735549927\n",
            "Epoch: 5000, Loss: 0.4915087819099426\n",
            "Epoch: 5010, Loss: 0.42622363567352295\n",
            "Epoch: 5020, Loss: 0.47565707564353943\n",
            "Epoch: 5030, Loss: 0.47921669483184814\n",
            "Epoch: 5040, Loss: 0.8715055584907532\n",
            "Epoch: 5050, Loss: 0.5600721836090088\n",
            "Epoch: 5060, Loss: 0.4535483121871948\n",
            "Epoch: 5070, Loss: 0.41146954894065857\n",
            "Epoch: 5080, Loss: 0.4109126627445221\n",
            "Epoch: 5090, Loss: 0.43792060017585754\n",
            "Epoch: 5100, Loss: 0.4142893850803375\n",
            "Epoch: 5110, Loss: 0.4132857024669647\n",
            "Epoch: 5120, Loss: 0.8258057236671448\n",
            "Epoch: 5130, Loss: 0.5778387188911438\n",
            "Epoch: 5140, Loss: 0.4588097035884857\n",
            "Epoch: 5150, Loss: 0.44856417179107666\n",
            "Epoch: 5160, Loss: 0.4412989914417267\n",
            "Epoch: 5170, Loss: 0.3921907842159271\n",
            "Epoch: 5180, Loss: 0.550956666469574\n",
            "Epoch: 5190, Loss: 0.40605247020721436\n",
            "Epoch: 5200, Loss: 0.45281723141670227\n",
            "Epoch: 5210, Loss: 0.4450463056564331\n",
            "Epoch: 5220, Loss: 0.4129945635795593\n",
            "Epoch: 5230, Loss: 0.45883792638778687\n",
            "Epoch: 5240, Loss: 0.5087557435035706\n",
            "Epoch: 5250, Loss: 0.46018654108047485\n",
            "Epoch: 5260, Loss: 0.44011998176574707\n",
            "Epoch: 5270, Loss: 0.4298500418663025\n",
            "Epoch: 5280, Loss: 0.4074511229991913\n",
            "Epoch: 5290, Loss: 0.5931289196014404\n",
            "Epoch: 5300, Loss: 0.423552542924881\n",
            "Epoch: 5310, Loss: 0.42230796813964844\n",
            "Epoch: 5320, Loss: 0.46758833527565\n",
            "Epoch: 5330, Loss: 0.471696138381958\n",
            "Epoch: 5340, Loss: 0.44866085052490234\n",
            "Epoch: 5350, Loss: 0.45892342925071716\n",
            "Epoch: 5360, Loss: 0.41982993483543396\n",
            "Epoch: 5370, Loss: 0.40666693449020386\n",
            "Epoch: 5380, Loss: 0.41910502314567566\n",
            "Epoch: 5390, Loss: 0.4900056719779968\n",
            "Epoch: 5400, Loss: 0.4550725519657135\n",
            "Epoch: 5410, Loss: 0.3875932991504669\n",
            "Epoch: 5420, Loss: 0.476165235042572\n",
            "Epoch: 5430, Loss: 0.9624274373054504\n",
            "Epoch: 5440, Loss: 0.43008652329444885\n",
            "Epoch: 5450, Loss: 0.39594021439552307\n",
            "Epoch: 5460, Loss: 0.44869959354400635\n",
            "Epoch: 5470, Loss: 0.5550802946090698\n",
            "Epoch: 5480, Loss: 0.463604211807251\n",
            "Epoch: 5490, Loss: 0.49498236179351807\n",
            "Epoch: 5500, Loss: 0.4716813564300537\n",
            "Epoch: 5510, Loss: 0.46670833230018616\n",
            "Epoch: 5520, Loss: 0.4388987720012665\n",
            "Epoch: 5530, Loss: 0.8302173018455505\n",
            "Epoch: 5540, Loss: 0.6966246962547302\n",
            "Epoch: 5550, Loss: 1.0110703706741333\n",
            "Epoch: 5560, Loss: 0.510291576385498\n",
            "Epoch: 5570, Loss: 0.45612865686416626\n",
            "Epoch: 5580, Loss: 0.8635708093643188\n",
            "Epoch: 5590, Loss: 0.49986016750335693\n",
            "Epoch: 5600, Loss: 0.41482505202293396\n",
            "Epoch: 5610, Loss: 0.43002697825431824\n",
            "Epoch: 5620, Loss: 0.3979244530200958\n",
            "Epoch: 5630, Loss: 0.40400514006614685\n",
            "Epoch: 5640, Loss: 0.3937439024448395\n",
            "Epoch: 5650, Loss: 0.41207411885261536\n",
            "Epoch: 5660, Loss: 0.3889187276363373\n",
            "Epoch: 5670, Loss: 0.43262043595314026\n",
            "Epoch: 5680, Loss: 0.4512736201286316\n",
            "Epoch: 5690, Loss: 0.4605734348297119\n",
            "Epoch: 5700, Loss: 0.42558765411376953\n",
            "Epoch: 5710, Loss: 0.40967491269111633\n",
            "Epoch: 5720, Loss: 0.5564149618148804\n",
            "Epoch: 5730, Loss: 0.4803076684474945\n",
            "Epoch: 5740, Loss: 0.41403430700302124\n",
            "Epoch: 5750, Loss: 0.5518298149108887\n",
            "Epoch: 5760, Loss: 0.4773269593715668\n",
            "Epoch: 5770, Loss: 0.48818331956863403\n",
            "Epoch: 5780, Loss: 0.4263986349105835\n",
            "Epoch: 5790, Loss: 0.43300873041152954\n",
            "Epoch: 5800, Loss: 0.46658214926719666\n",
            "Epoch: 5810, Loss: 0.47006386518478394\n",
            "Epoch: 5820, Loss: 0.525714099407196\n",
            "Epoch: 5830, Loss: 0.3727779686450958\n",
            "Epoch: 5840, Loss: 1.0158090591430664\n",
            "Epoch: 5850, Loss: 0.41954004764556885\n",
            "Epoch: 5860, Loss: 0.5374339818954468\n",
            "Epoch: 5870, Loss: 0.5828625559806824\n",
            "Epoch: 5880, Loss: 0.4442633092403412\n",
            "Epoch: 5890, Loss: 0.631674587726593\n",
            "Epoch: 5900, Loss: 0.37975990772247314\n",
            "Epoch: 5910, Loss: 0.9723346829414368\n",
            "Epoch: 5920, Loss: 0.43950173258781433\n",
            "Epoch: 5930, Loss: 0.3917614817619324\n",
            "Epoch: 5940, Loss: 0.4807751774787903\n",
            "Epoch: 5950, Loss: 0.4159499704837799\n",
            "Epoch: 5960, Loss: 0.46389445662498474\n",
            "Epoch: 5970, Loss: 0.4572199285030365\n",
            "Epoch: 5980, Loss: 0.7209993004798889\n",
            "Epoch: 5990, Loss: 0.43075031042099\n",
            "Epoch: 6000, Loss: 0.4926272928714752\n",
            "Epoch: 6010, Loss: 0.38658660650253296\n",
            "Epoch: 6020, Loss: 0.41544121503829956\n",
            "Epoch: 6030, Loss: 0.3925999402999878\n",
            "Epoch: 6040, Loss: 0.40320926904678345\n",
            "Epoch: 6050, Loss: 0.47984713315963745\n",
            "Epoch: 6060, Loss: 0.46830353140830994\n",
            "Epoch: 6070, Loss: 0.4157903790473938\n",
            "Epoch: 6080, Loss: 0.4375285804271698\n",
            "Epoch: 6090, Loss: 0.9813884496688843\n",
            "Epoch: 6100, Loss: 0.40879181027412415\n",
            "Epoch: 6110, Loss: 0.39377743005752563\n",
            "Epoch: 6120, Loss: 0.4442863166332245\n",
            "Epoch: 6130, Loss: 0.38010379672050476\n",
            "Epoch: 6140, Loss: 0.4418753385543823\n",
            "Epoch: 6150, Loss: 0.594099223613739\n",
            "Epoch: 6160, Loss: 0.749754011631012\n",
            "Epoch: 6170, Loss: 0.46101370453834534\n",
            "Epoch: 6180, Loss: 1.068374514579773\n",
            "Epoch: 6190, Loss: 0.4907793402671814\n",
            "Epoch: 6200, Loss: 0.5021361112594604\n",
            "Epoch: 6210, Loss: 0.44950154423713684\n",
            "Epoch: 6220, Loss: 0.438264936208725\n",
            "Epoch: 6230, Loss: 0.4242629110813141\n",
            "Epoch: 6240, Loss: 0.4611537456512451\n",
            "Epoch: 6250, Loss: 0.4381530284881592\n",
            "Epoch: 6260, Loss: 0.4109845757484436\n",
            "Epoch: 6270, Loss: 0.48391973972320557\n",
            "Epoch: 6280, Loss: 0.4003630578517914\n",
            "Epoch: 6290, Loss: 0.4563787579536438\n",
            "Epoch: 6300, Loss: 0.45595791935920715\n",
            "Epoch: 6310, Loss: 0.5121670365333557\n",
            "Epoch: 6320, Loss: 0.4336487054824829\n",
            "Epoch: 6330, Loss: 0.46521812677383423\n",
            "Epoch: 6340, Loss: 0.4302339553833008\n",
            "Epoch: 6350, Loss: 0.38950833678245544\n",
            "Epoch: 6360, Loss: 0.40516555309295654\n",
            "Epoch: 6370, Loss: 0.422250360250473\n",
            "Epoch: 6380, Loss: 0.39285361766815186\n",
            "Epoch: 6390, Loss: 0.42963340878486633\n",
            "Epoch: 6400, Loss: 0.5170436501502991\n",
            "Epoch: 6410, Loss: 0.4675312936306\n",
            "Epoch: 6420, Loss: 0.5924655199050903\n",
            "Epoch: 6430, Loss: 0.4187450408935547\n",
            "Epoch: 6440, Loss: 0.4719264507293701\n",
            "Epoch: 6450, Loss: 0.4208322763442993\n",
            "Epoch: 6460, Loss: 0.4664383828639984\n",
            "Epoch: 6470, Loss: 0.43368709087371826\n",
            "Epoch: 6480, Loss: 0.4027418792247772\n",
            "Epoch: 6490, Loss: 0.36582428216934204\n",
            "Epoch: 6500, Loss: 0.44920212030410767\n",
            "Epoch: 6510, Loss: 0.838737428188324\n",
            "Epoch: 6520, Loss: 0.5781248807907104\n",
            "Epoch: 6530, Loss: 0.42585739493370056\n",
            "Epoch: 6540, Loss: 0.4344227612018585\n",
            "Epoch: 6550, Loss: 0.5589596033096313\n",
            "Epoch: 6560, Loss: 0.41483840346336365\n",
            "Epoch: 6570, Loss: 0.49841946363449097\n",
            "Epoch: 6580, Loss: 0.441543310880661\n",
            "Epoch: 6590, Loss: 0.5607131719589233\n",
            "Epoch: 6600, Loss: 0.5431948304176331\n",
            "Epoch: 6610, Loss: 0.5086860060691833\n",
            "Epoch: 6620, Loss: 0.48584938049316406\n",
            "Epoch: 6630, Loss: 0.477921724319458\n",
            "Epoch: 6640, Loss: 0.5034591555595398\n",
            "Epoch: 6650, Loss: 0.49376538395881653\n",
            "Epoch: 6660, Loss: 0.45520538091659546\n",
            "Epoch: 6670, Loss: 0.4387539029121399\n",
            "Epoch: 6680, Loss: 0.4428694546222687\n",
            "Epoch: 6690, Loss: 0.4500402808189392\n",
            "Epoch: 6700, Loss: 0.503496527671814\n",
            "Epoch: 6710, Loss: 0.43872299790382385\n",
            "Epoch: 6720, Loss: 0.447489470243454\n",
            "Epoch: 6730, Loss: 0.3841506540775299\n",
            "Epoch: 6740, Loss: 0.8377435803413391\n",
            "Epoch: 6750, Loss: 0.5154711008071899\n",
            "Epoch: 6760, Loss: 0.45526739954948425\n",
            "Epoch: 6770, Loss: 0.4857581853866577\n",
            "Epoch: 6780, Loss: 0.6146491169929504\n",
            "Epoch: 6790, Loss: 0.39275091886520386\n",
            "Epoch: 6800, Loss: 0.417946457862854\n",
            "Epoch: 6810, Loss: 0.9326338171958923\n",
            "Epoch: 6820, Loss: 0.3989456295967102\n",
            "Epoch: 6830, Loss: 0.42632684111595154\n",
            "Epoch: 6840, Loss: 0.3891150653362274\n",
            "Epoch: 6850, Loss: 0.5437521934509277\n",
            "Epoch: 6860, Loss: 0.4069426655769348\n",
            "Epoch: 6870, Loss: 0.5423553586006165\n",
            "Epoch: 6880, Loss: 0.47481438517570496\n",
            "Epoch: 6890, Loss: 0.4304722547531128\n",
            "Epoch: 6900, Loss: 0.4235341548919678\n",
            "Epoch: 6910, Loss: 0.48228955268859863\n",
            "Epoch: 6920, Loss: 0.7553389668464661\n",
            "Epoch: 6930, Loss: 0.5442401170730591\n",
            "Epoch: 6940, Loss: 0.6253326535224915\n",
            "Epoch: 6950, Loss: 0.45751404762268066\n",
            "Epoch: 6960, Loss: 0.44834136962890625\n",
            "Epoch: 6970, Loss: 0.4577711224555969\n",
            "Epoch: 6980, Loss: 0.4541943669319153\n",
            "Epoch: 6990, Loss: 0.4383828639984131\n",
            "Epoch: 7000, Loss: 0.43838226795196533\n",
            "Epoch: 7010, Loss: 0.4514237940311432\n",
            "Epoch: 7020, Loss: 0.41699832677841187\n",
            "Epoch: 7030, Loss: 0.5113593339920044\n",
            "Epoch: 7040, Loss: 0.651084303855896\n",
            "Epoch: 7050, Loss: 0.4566843509674072\n",
            "Epoch: 7060, Loss: 0.4000321626663208\n",
            "Epoch: 7070, Loss: 0.41327595710754395\n",
            "Epoch: 7080, Loss: 0.5516320466995239\n",
            "Epoch: 7090, Loss: 0.4138648211956024\n",
            "Epoch: 7100, Loss: 0.45248621702194214\n",
            "Epoch: 7110, Loss: 0.4637250602245331\n",
            "Epoch: 7120, Loss: 0.43572482466697693\n",
            "Epoch: 7130, Loss: 0.4325312077999115\n",
            "Epoch: 7140, Loss: 0.455485463142395\n",
            "Epoch: 7150, Loss: 0.40659099817276\n",
            "Epoch: 7160, Loss: 0.4284301698207855\n",
            "Epoch: 7170, Loss: 0.39516451954841614\n",
            "Epoch: 7180, Loss: 0.4232998788356781\n",
            "Epoch: 7190, Loss: 0.42057856917381287\n",
            "Epoch: 7200, Loss: 1.0183247327804565\n",
            "Epoch: 7210, Loss: 0.4534021317958832\n",
            "Epoch: 7220, Loss: 0.4537234902381897\n",
            "Epoch: 7230, Loss: 0.40101268887519836\n",
            "Epoch: 7240, Loss: 0.5010722279548645\n",
            "Epoch: 7250, Loss: 0.4167156219482422\n",
            "Epoch: 7260, Loss: 0.4982450008392334\n",
            "Epoch: 7270, Loss: 0.5077539682388306\n",
            "Epoch: 7280, Loss: 0.43087196350097656\n",
            "Epoch: 7290, Loss: 0.407265305519104\n",
            "Epoch: 7300, Loss: 0.5633379220962524\n",
            "Epoch: 7310, Loss: 0.4425813555717468\n",
            "Epoch: 7320, Loss: 0.40136417746543884\n",
            "Epoch: 7330, Loss: 0.6894795298576355\n",
            "Epoch: 7340, Loss: 0.45631420612335205\n",
            "Epoch: 7350, Loss: 0.4204367697238922\n",
            "Epoch: 7360, Loss: 0.5255953073501587\n",
            "Epoch: 7370, Loss: 0.4485691785812378\n",
            "Epoch: 7380, Loss: 0.4026588201522827\n",
            "Epoch: 7390, Loss: 0.4271581172943115\n",
            "Epoch: 7400, Loss: 0.3965414762496948\n",
            "Epoch: 7410, Loss: 0.4377177059650421\n",
            "Epoch: 7420, Loss: 0.3659785985946655\n",
            "Epoch: 7430, Loss: 0.4646678566932678\n",
            "Epoch: 7440, Loss: 0.46618038415908813\n",
            "Epoch: 7450, Loss: 0.43012577295303345\n",
            "Epoch: 7460, Loss: 0.5158529877662659\n",
            "Epoch: 7470, Loss: 0.46409597992897034\n",
            "Epoch: 7480, Loss: 0.440985769033432\n",
            "Epoch: 7490, Loss: 0.5070348978042603\n",
            "Epoch: 7500, Loss: 0.49773091077804565\n",
            "Epoch: 7510, Loss: 0.5296013951301575\n",
            "Epoch: 7520, Loss: 0.4528394043445587\n",
            "Epoch: 7530, Loss: 0.8148941993713379\n",
            "Epoch: 7540, Loss: 0.4473554491996765\n",
            "Epoch: 7550, Loss: 0.4625227749347687\n",
            "Epoch: 7560, Loss: 0.46216148138046265\n",
            "Epoch: 7570, Loss: 0.4297471344470978\n",
            "Epoch: 7580, Loss: 0.45314982533454895\n",
            "Epoch: 7590, Loss: 0.4330170750617981\n",
            "Epoch: 7600, Loss: 0.42029473185539246\n",
            "Epoch: 7610, Loss: 0.6260320544242859\n",
            "Epoch: 7620, Loss: 0.4278135299682617\n",
            "Epoch: 7630, Loss: 0.470925509929657\n",
            "Epoch: 7640, Loss: 0.914571225643158\n",
            "Epoch: 7650, Loss: 0.6171500086784363\n",
            "Epoch: 7660, Loss: 0.49245181679725647\n",
            "Epoch: 7670, Loss: 0.41696029901504517\n",
            "Epoch: 7680, Loss: 0.6366938948631287\n",
            "Epoch: 7690, Loss: 0.44377273321151733\n",
            "Epoch: 7700, Loss: 0.45184773206710815\n",
            "Epoch: 7710, Loss: 0.5999542474746704\n",
            "Epoch: 7720, Loss: 0.406698077917099\n",
            "Epoch: 7730, Loss: 0.41065654158592224\n",
            "Epoch: 7740, Loss: 0.36612579226493835\n",
            "Epoch: 7750, Loss: 0.39719945192337036\n",
            "Epoch: 7760, Loss: 0.4734468162059784\n",
            "Epoch: 7770, Loss: 0.42275410890579224\n",
            "Epoch: 7780, Loss: 0.430166095495224\n",
            "Epoch: 7790, Loss: 0.4498014748096466\n",
            "Epoch: 7800, Loss: 0.4338420629501343\n",
            "Epoch: 7810, Loss: 0.43756264448165894\n",
            "Epoch: 7820, Loss: 0.5574809908866882\n",
            "Epoch: 7830, Loss: 0.4229929447174072\n",
            "Epoch: 7840, Loss: 0.45862439274787903\n",
            "Epoch: 7850, Loss: 0.43770644068717957\n",
            "Epoch: 7860, Loss: 0.4004734754562378\n",
            "Epoch: 7870, Loss: 0.5785073637962341\n",
            "Epoch: 7880, Loss: 0.40068739652633667\n",
            "Epoch: 7890, Loss: 0.3908903896808624\n",
            "Epoch: 7900, Loss: 0.6877108812332153\n",
            "Epoch: 7910, Loss: 0.43670326471328735\n",
            "Epoch: 7920, Loss: 0.39490607380867004\n",
            "Epoch: 7930, Loss: 0.4903985857963562\n",
            "Epoch: 7940, Loss: 0.46293655037879944\n",
            "Epoch: 7950, Loss: 0.4229643642902374\n",
            "Epoch: 7960, Loss: 0.43925124406814575\n",
            "Epoch: 7970, Loss: 0.40722209215164185\n",
            "Epoch: 7980, Loss: 0.4096449315547943\n",
            "Epoch: 7990, Loss: 0.3823033571243286\n",
            "Epoch: 8000, Loss: 0.4913775622844696\n",
            "Epoch: 8010, Loss: 0.42293858528137207\n",
            "Epoch: 8020, Loss: 0.4511652886867523\n",
            "Epoch: 8030, Loss: 0.491733193397522\n",
            "Epoch: 8040, Loss: 0.8936957716941833\n",
            "Epoch: 8050, Loss: 0.4503382444381714\n",
            "Epoch: 8060, Loss: 0.5030495524406433\n",
            "Epoch: 8070, Loss: 1.1175706386566162\n",
            "Epoch: 8080, Loss: 0.4779953360557556\n",
            "Epoch: 8090, Loss: 0.478638619184494\n",
            "Epoch: 8100, Loss: 0.5378087759017944\n",
            "Epoch: 8110, Loss: 0.5315238833427429\n",
            "Epoch: 8120, Loss: 0.4492446482181549\n",
            "Epoch: 8130, Loss: 0.411075621843338\n",
            "Epoch: 8140, Loss: 0.47134649753570557\n",
            "Epoch: 8150, Loss: 0.43022042512893677\n",
            "Epoch: 8160, Loss: 0.42777568101882935\n",
            "Epoch: 8170, Loss: 0.37106460332870483\n",
            "Epoch: 8180, Loss: 0.3967002034187317\n",
            "Epoch: 8190, Loss: 0.4625846743583679\n",
            "Epoch: 8200, Loss: 0.3902492821216583\n",
            "Epoch: 8210, Loss: 0.3996495008468628\n",
            "Epoch: 8220, Loss: 0.5047573447227478\n",
            "Epoch: 8230, Loss: 0.38192036747932434\n",
            "Epoch: 8240, Loss: 0.4144207835197449\n",
            "Epoch: 8250, Loss: 0.48134174942970276\n",
            "Epoch: 8260, Loss: 0.4345640242099762\n",
            "Epoch: 8270, Loss: 0.43132027983665466\n",
            "Epoch: 8280, Loss: 0.4335804879665375\n",
            "Epoch: 8290, Loss: 0.4300551116466522\n",
            "Epoch: 8300, Loss: 0.6463321447372437\n",
            "Epoch: 8310, Loss: 0.4961376190185547\n",
            "Epoch: 8320, Loss: 0.7607879042625427\n",
            "Epoch: 8330, Loss: 0.4608006775379181\n",
            "Epoch: 8340, Loss: 0.4701317846775055\n",
            "Epoch: 8350, Loss: 1.177292823791504\n",
            "Epoch: 8360, Loss: 0.5224521160125732\n",
            "Epoch: 8370, Loss: 0.7042158246040344\n",
            "Epoch: 8380, Loss: 0.5471254587173462\n",
            "Epoch: 8390, Loss: 0.46820068359375\n",
            "Epoch: 8400, Loss: 0.3995472192764282\n",
            "Epoch: 8410, Loss: 0.5128132104873657\n",
            "Epoch: 8420, Loss: 0.41639673709869385\n",
            "Epoch: 8430, Loss: 0.45208144187927246\n",
            "Epoch: 8440, Loss: 0.42436638474464417\n",
            "Epoch: 8450, Loss: 0.4883648753166199\n",
            "Epoch: 8460, Loss: 0.4205031096935272\n",
            "Epoch: 8470, Loss: 0.7024819850921631\n",
            "Epoch: 8480, Loss: 0.4214751124382019\n",
            "Epoch: 8490, Loss: 0.45426174998283386\n",
            "Epoch: 8500, Loss: 0.42954012751579285\n",
            "Epoch: 8510, Loss: 0.44237813353538513\n",
            "Epoch: 8520, Loss: 0.3859143853187561\n",
            "Epoch: 8530, Loss: 0.4169289469718933\n",
            "Epoch: 8540, Loss: 0.5679265260696411\n",
            "Epoch: 8550, Loss: 0.4693237245082855\n",
            "Epoch: 8560, Loss: 0.39050838351249695\n",
            "Epoch: 8570, Loss: 0.4194067716598511\n",
            "Epoch: 8580, Loss: 0.40683504939079285\n",
            "Epoch: 8590, Loss: 0.411329984664917\n",
            "Epoch: 8600, Loss: 0.40148916840553284\n",
            "Epoch: 8610, Loss: 0.4072265923023224\n",
            "Epoch: 8620, Loss: 0.5173476338386536\n",
            "Epoch: 8630, Loss: 0.5772382616996765\n",
            "Epoch: 8640, Loss: 0.37794792652130127\n",
            "Epoch: 8650, Loss: 0.37113818526268005\n",
            "Epoch: 8660, Loss: 0.6651853322982788\n",
            "Epoch: 8670, Loss: 0.6091735363006592\n",
            "Epoch: 8680, Loss: 0.4346984028816223\n",
            "Epoch: 8690, Loss: 0.4580520689487457\n",
            "Epoch: 8700, Loss: 0.4403863251209259\n",
            "Epoch: 8710, Loss: 0.44866108894348145\n",
            "Epoch: 8720, Loss: 0.9126535058021545\n",
            "Epoch: 8730, Loss: 0.41251060366630554\n",
            "Epoch: 8740, Loss: 0.45113512873649597\n",
            "Epoch: 8750, Loss: 0.4055473804473877\n",
            "Epoch: 8760, Loss: 0.40512219071388245\n",
            "Epoch: 8770, Loss: 0.5182666182518005\n",
            "Epoch: 8780, Loss: 0.40681320428848267\n",
            "Epoch: 8790, Loss: 0.5039111971855164\n",
            "Epoch: 8800, Loss: 0.4472096264362335\n",
            "Epoch: 8810, Loss: 0.44933411478996277\n",
            "Epoch: 8820, Loss: 0.4009787440299988\n",
            "Epoch: 8830, Loss: 0.40567073225975037\n",
            "Epoch: 8840, Loss: 0.4266757071018219\n",
            "Epoch: 8850, Loss: 0.45779135823249817\n",
            "Epoch: 8860, Loss: 0.44943901896476746\n",
            "Epoch: 8870, Loss: 0.49596959352493286\n",
            "Epoch: 8880, Loss: 0.4552386403083801\n",
            "Epoch: 8890, Loss: 0.4655139148235321\n",
            "Epoch: 8900, Loss: 0.39700043201446533\n",
            "Epoch: 8910, Loss: 0.41421806812286377\n",
            "Epoch: 8920, Loss: 0.40168026089668274\n",
            "Epoch: 8930, Loss: 0.39367973804473877\n",
            "Epoch: 8940, Loss: 0.7073081731796265\n",
            "Epoch: 8950, Loss: 0.398863285779953\n",
            "Epoch: 8960, Loss: 0.41250213980674744\n",
            "Epoch: 8970, Loss: 0.44909805059432983\n",
            "Epoch: 8980, Loss: 0.41307541728019714\n",
            "Epoch: 8990, Loss: 0.3601660430431366\n",
            "Epoch: 9000, Loss: 0.3932242691516876\n",
            "Epoch: 9010, Loss: 0.46296975016593933\n",
            "Epoch: 9020, Loss: 0.4013911187648773\n",
            "Epoch: 9030, Loss: 0.405868798494339\n",
            "Epoch: 9040, Loss: 0.9334248304367065\n",
            "Epoch: 9050, Loss: 0.46826255321502686\n",
            "Epoch: 9060, Loss: 0.7158017158508301\n",
            "Epoch: 9070, Loss: 0.4808742105960846\n",
            "Epoch: 9080, Loss: 1.0049188137054443\n",
            "Epoch: 9090, Loss: 0.48646271228790283\n",
            "Epoch: 9100, Loss: 0.48452162742614746\n",
            "Epoch: 9110, Loss: 0.4775923490524292\n",
            "Epoch: 9120, Loss: 0.5152175426483154\n",
            "Epoch: 9130, Loss: 0.43517303466796875\n",
            "Epoch: 9140, Loss: 0.3917196989059448\n",
            "Epoch: 9150, Loss: 0.3804694414138794\n",
            "Epoch: 9160, Loss: 0.42518067359924316\n",
            "Epoch: 9170, Loss: 0.4856574237346649\n",
            "Epoch: 9180, Loss: 0.4094616770744324\n",
            "Epoch: 9190, Loss: 0.39588871598243713\n",
            "Epoch: 9200, Loss: 0.4012519121170044\n",
            "Epoch: 9210, Loss: 0.4118450880050659\n",
            "Epoch: 9220, Loss: 0.46793800592422485\n",
            "Epoch: 9230, Loss: 0.42840683460235596\n",
            "Epoch: 9240, Loss: 0.39440518617630005\n",
            "Epoch: 9250, Loss: 0.41272222995758057\n",
            "Epoch: 9260, Loss: 0.43312159180641174\n",
            "Epoch: 9270, Loss: 0.3915533423423767\n",
            "Epoch: 9280, Loss: 0.4426616430282593\n",
            "Epoch: 9290, Loss: 0.5604206919670105\n",
            "Epoch: 9300, Loss: 0.5643985867500305\n",
            "Epoch: 9310, Loss: 0.38164258003234863\n",
            "Epoch: 9320, Loss: 0.40698960423469543\n",
            "Epoch: 9330, Loss: 0.4075378179550171\n",
            "Epoch: 9340, Loss: 0.44412821531295776\n",
            "Epoch: 9350, Loss: 0.42438027262687683\n",
            "Epoch: 9360, Loss: 0.4919743835926056\n",
            "Epoch: 9370, Loss: 0.475473552942276\n",
            "Epoch: 9380, Loss: 0.49283912777900696\n",
            "Epoch: 9390, Loss: 0.4580543637275696\n",
            "Epoch: 9400, Loss: 0.38921067118644714\n",
            "Epoch: 9410, Loss: 0.7155482769012451\n",
            "Epoch: 9420, Loss: 0.6917697191238403\n",
            "Epoch: 9430, Loss: 0.5413292050361633\n",
            "Epoch: 9440, Loss: 0.48097267746925354\n",
            "Epoch: 9450, Loss: 0.4510166049003601\n",
            "Epoch: 9460, Loss: 0.4067714810371399\n",
            "Epoch: 9470, Loss: 0.3891736567020416\n",
            "Epoch: 9480, Loss: 0.4072631001472473\n",
            "Epoch: 9490, Loss: 0.5297843813896179\n",
            "Epoch: 9500, Loss: 0.5956425070762634\n",
            "Epoch: 9510, Loss: 0.5088936686515808\n",
            "Epoch: 9520, Loss: 0.4208948016166687\n",
            "Epoch: 9530, Loss: 0.41472041606903076\n",
            "Epoch: 9540, Loss: 0.3917829394340515\n",
            "Epoch: 9550, Loss: 0.3733570873737335\n",
            "Epoch: 9560, Loss: 0.4927734136581421\n",
            "Epoch: 9570, Loss: 0.8189024329185486\n",
            "Epoch: 9580, Loss: 0.41617316007614136\n",
            "Epoch: 9590, Loss: 0.5332800149917603\n",
            "Epoch: 9600, Loss: 0.554089367389679\n",
            "Epoch: 9610, Loss: 0.41283923387527466\n",
            "Epoch: 9620, Loss: 0.44219279289245605\n",
            "Epoch: 9630, Loss: 0.44164231419563293\n",
            "Epoch: 9640, Loss: 0.4608146548271179\n",
            "Epoch: 9650, Loss: 0.4928615987300873\n",
            "Epoch: 9660, Loss: 0.45700180530548096\n",
            "Epoch: 9670, Loss: 0.40083158016204834\n",
            "Epoch: 9680, Loss: 0.4230526089668274\n",
            "Epoch: 9690, Loss: 1.0141633749008179\n",
            "Epoch: 9700, Loss: 0.44975700974464417\n",
            "Epoch: 9710, Loss: 0.40646132826805115\n",
            "Epoch: 9720, Loss: 0.46943777799606323\n",
            "Epoch: 9730, Loss: 0.4774724245071411\n",
            "Epoch: 9740, Loss: 0.4698924720287323\n",
            "Epoch: 9750, Loss: 1.0235400199890137\n",
            "Epoch: 9760, Loss: 0.4834359884262085\n",
            "Epoch: 9770, Loss: 0.5032320022583008\n",
            "Epoch: 9780, Loss: 0.4243868887424469\n",
            "Epoch: 9790, Loss: 0.46504077315330505\n",
            "Epoch: 9800, Loss: 0.4519662857055664\n",
            "Epoch: 9810, Loss: 0.6960764527320862\n",
            "Epoch: 9820, Loss: 0.4766944646835327\n",
            "Epoch: 9830, Loss: 0.5832633376121521\n",
            "Epoch: 9840, Loss: 0.4776776134967804\n",
            "Epoch: 9850, Loss: 0.44315171241760254\n",
            "Epoch: 9860, Loss: 0.3862572908401489\n",
            "Epoch: 9870, Loss: 0.4657854437828064\n",
            "Epoch: 9880, Loss: 0.46607890725135803\n",
            "Epoch: 9890, Loss: 0.5190607905387878\n",
            "Epoch: 9900, Loss: 0.44037696719169617\n",
            "Epoch: 9910, Loss: 0.38769248127937317\n",
            "Epoch: 9920, Loss: 0.3822519779205322\n",
            "Epoch: 9930, Loss: 0.3806384205818176\n",
            "Epoch: 9940, Loss: 0.500723123550415\n",
            "Epoch: 9950, Loss: 0.39264070987701416\n",
            "Epoch: 9960, Loss: 0.47051459550857544\n",
            "Epoch: 9970, Loss: 0.4219142496585846\n",
            "Epoch: 9980, Loss: 0.49511587619781494\n",
            "Epoch: 9990, Loss: 0.4408033490180969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPLGR7Wz12gS"
      },
      "source": [
        "Шаблон функции `generate_sample` также доступен ниже. Вы можете как дозаполнить его, так и написать свою собственную функцию с нуля. Не забывайте, что все примеры в обучающей выборке начинались с токена `<sos>`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sample(char_rnn, seed_phrase=None, max_length=500, temperature=1.0, device='cuda'):\n",
        "    char_rnn.eval()\n",
        "\n",
        "    if seed_phrase is not None:\n",
        "        x_sequence = [token_to_idx[token] for token in seed_phrase]\n",
        "    else:\n",
        "        x_sequence = []\n",
        "\n",
        "    x_sequence = [token_to_idx['<sos>']] + x_sequence\n",
        "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64).to(device)\n",
        "\n",
        "    hidden = None\n",
        "\n",
        "    while len(x_sequence[0]) - 1 < max_length:\n",
        "        output, hidden = char_rnn(x_sequence[:, -1].view(1, 1), hidden)\n",
        "\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "        x_sequence = torch.cat([x_sequence, top_i.view(1, 1)], dim=1)\n",
        "\n",
        "    generated_sequence = ''.join([tokens[ix] for ix in x_sequence.cpu().data.numpy()[0]])\n",
        "\n",
        "    return generated_sequence.replace('<sos>', '', 1)[:max_length]\n",
        "\n"
      ],
      "metadata": {
        "id": "5XOlgsclnNlh"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if0Zdsq_12gS"
      },
      "source": [
        "Пример текста сгенерированного обученной моделью доступен ниже. Не страшно, что в тексте много несуществующих слов. Используемая модель очень проста: это простая классическая RNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "yF-Iuqmw12gS",
        "outputId": "b0d58d9f-0e0c-441e-e6c3-d4440c3ba3ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " мой дядя самых честных правили;\n",
            "глазамет там беседогольные ларин,\n",
            "что ее ручей\n",
            "онегин с изветы, три буян,\n",
            "как зеленья\n",
            "взять об одном\n",
            "и день далеком\n",
            "ей черернул ее сердце обыта: меж долго в каши двух стол ж из-циила тишин,\n",
            "унесть одно и то же квакать,\n",
            "жалеть о прежнем, о бына\n",
            "на нос и занесены,\n",
            "любил бы всем стара ж его боят, небеснят награль.\n",
            "«poor yorick! – молвил он любим… предмете погруженных,\n",
            "ш остеклет мас. nii\n",
            "\n",
            "под финского поток\n",
            "заветный вновь примирать,\n",
            "брать и чет вечер длинный ножки! \n"
          ]
        }
      ],
      "source": [
        "print(generate_sample(model, ' мой дядя самых честных правил', max_length=500, temperature=0.8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwFvHiKP12gS"
      },
      "source": [
        "### Сдача задания\n",
        "Сгенерируйте десять последовательностей длиной 500, используя строку ' мой дядя самых честных правил'. Температуру для генерации выберите самостоятельно на основании визуального качества генериуремого текста. Не забудьте удалить все технические токены в случае их наличия.\n",
        "\n",
        "Сгенерированную последовательность сохрание в переменную `generated_phrase` и сдайте сгенерированный ниже файл в контест."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "sB5p9j9T12gT"
      },
      "outputs": [],
      "source": [
        "seed_phrase = ' мой дядя самых честных правил'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "Qe9T5-Op12gT"
      },
      "outputs": [],
      "source": [
        "generated_phrases = [\n",
        "    generate_sample(\n",
        "        model,\n",
        "        seed_phrase,\n",
        "        max_length=500,\n",
        "        temperature=0.8\n",
        "    )\n",
        "    for _ in range(10)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "kKTbJtIi12gT",
        "outputId": "f47a0376-ef65-4486-ce5e-b676d40e0984",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved to `submission_dict_hw07.npy`\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "if 'generated_phrases' not in locals():\n",
        "    raise ValueError(\"Please, save generated phrases to `generated_phrases` variable\")\n",
        "\n",
        "for phrase in generated_phrases:\n",
        "\n",
        "    if not isinstance(phrase, str):\n",
        "        raise ValueError(\"The generated phrase should be a string\")\n",
        "\n",
        "    if len(phrase) != 500:\n",
        "        raise ValueError(\"The `generated_phrase` length should be equal to 500\")\n",
        "\n",
        "    assert all([x in set(tokens) for x in set(list(phrase))]), 'Unknown tokens detected, check your submission!'\n",
        "\n",
        "\n",
        "submission_dict = {\n",
        "    'token_to_idx': token_to_idx,\n",
        "    'generated_phrases': generated_phrases\n",
        "}\n",
        "\n",
        "np.save('submission_dict_hw07.npy', submission_dict, allow_pickle=True)\n",
        "print('File saved to `submission_dict_hw07.npy`')\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7ONIO0612gT"
      },
      "source": [
        "На этом задание завершено. Поздравляем!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Py3 Research",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}